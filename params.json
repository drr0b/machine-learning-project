{"name":"Machine-learning-project","tagline":"","body":"\r\n\r\n<div id=\"machine-learning-coursework\" class=\"section level1\">\r\n<h1>Machine Learning Coursework</h1>\r\n<div id=\"introduction\" class=\"section level2\">\r\n<h2>Introduction</h2>\r\n<p>In this work, we aim to predict the activities undertaken by subjects based on accelerometerdata provided by the human activity recognition group (HAR): <a href=\"http://groupware.les.inf.puc-rio.br/har\" class=\"uri\">http://groupware.les.inf.puc-rio.br/har</a>. The training data concerns 6 subjects performing 5 activities (A,B,C,D,E), some correct and some incorrect. Accelerometer data include X-, Y-, Z- movement, pitch, yaw, and roll for sensors on different parts of the body.</p>\r\n<p>Machine learning models were built from this training data in order to predict the activities in 20 tests, based on data from accelerometers, gyroscopes etc. alone.</p>\r\n</div>\r\n<div id=\"reading-in-data\" class=\"section level2\">\r\n<h2>Reading in data</h2>\r\n<p>We begin by reading in the provided data.</p>\r\n<pre class=\"r\"><code>training &lt;- read.csv(&quot;pml-training.csv&quot;)\r\ntesting &lt;- read.csv(&quot;pml-testing.csv&quot;)</code></pre>\r\n<p>Many of the columns are empty or NA except for when a new window starts, while others give information such as the name of the subject, and the time of the activity. The test data do not have the information provided in the new window columns. Useful columns seem to be 8-11, 37-49, 60-68, 84-86, 113-124, 151-160.</p>\r\n</div>\r\n<div id=\"building-the-model\" class=\"section level2\">\r\n<h2>Building the model</h2>\r\n<div id=\"initial-work\" class=\"section level3\">\r\n<h3>Initial work</h3>\r\n<p>Given that there were 6 subjects, a natural method for cross validation is to predict on one subject and test it on other subjects in the training data. The first subject is Carlitos, so we build the first model with his data.</p>\r\n<p>We use a random forest model, which makes numerous decision trees for selecting the class, then votes on the best outcome for each set of predictors.</p>\r\n<pre class=\"r\"><code>library(caret)</code></pre>\r\n<pre><code>## Warning: package 'caret' was built under R version 3.2.2</code></pre>\r\n<pre><code>## Loading required package: lattice\r\n## Loading required package: ggplot2</code></pre>\r\n<pre class=\"r\"><code>set.seed(123456)\r\ncarlitos &lt;- training[training$user_name==&quot;carlitos&quot;,c(8:11,37:49,60:68,84:86,113:124,151:160)]\r\nmodFit1 &lt;- train(factor(classe)~.,method=&quot;rf&quot;,data=carlitos)</code></pre>\r\n<pre><code>## Loading required package: randomForest</code></pre>\r\n<pre><code>## Warning: package 'randomForest' was built under R version 3.2.2</code></pre>\r\n<pre><code>## randomForest 4.6-12\r\n## Type rfNews() to see new features/changes/bug fixes.</code></pre>\r\n<pre class=\"r\"><code># Save model here to avoid re running it in multiple compiles\r\nsaveRDS(modFit1,file=&quot;mod1&quot;)\r\nmodFit1&lt;- readRDS(&quot;mod1&quot;)</code></pre>\r\n<p>We compare the predicted values of each excercise with the \"classe\" variable.</p>\r\n<pre class=\"r\"><code>prediction1 &lt;- predict(modFit1, carlitos)\r\nconfusionMatrix(carlitos$classe,prediction1)</code></pre>\r\n<pre><code>## Confusion Matrix and Statistics\r\n## \r\n##           Reference\r\n## Prediction   A   B   C   D   E\r\n##          A 834   0   0   0   0\r\n##          B   0 690   0   0   0\r\n##          C   0   0 493   0   0\r\n##          D   0   0   0 486   0\r\n##          E   0   0   0   0 609\r\n## \r\n## Overall Statistics\r\n##                                      \r\n##                Accuracy : 1          \r\n##                  95% CI : (0.9988, 1)\r\n##     No Information Rate : 0.268      \r\n##     P-Value [Acc &gt; NIR] : &lt; 2.2e-16  \r\n##                                      \r\n##                   Kappa : 1          \r\n##  Mcnemar's Test P-Value : NA         \r\n## \r\n## Statistics by Class:\r\n## \r\n##                      Class: A Class: B Class: C Class: D Class: E\r\n## Sensitivity             1.000   1.0000   1.0000   1.0000   1.0000\r\n## Specificity             1.000   1.0000   1.0000   1.0000   1.0000\r\n## Pos Pred Value          1.000   1.0000   1.0000   1.0000   1.0000\r\n## Neg Pred Value          1.000   1.0000   1.0000   1.0000   1.0000\r\n## Prevalence              0.268   0.2217   0.1584   0.1562   0.1957\r\n## Detection Rate          0.268   0.2217   0.1584   0.1562   0.1957\r\n## Detection Prevalence    0.268   0.2217   0.1584   0.1562   0.1957\r\n## Balanced Accuracy       1.000   1.0000   1.0000   1.0000   1.0000</code></pre>\r\n<p>We see the prediction to be near-perfect, but random forests are prone to overfitting. When we compare with Eurico's data, we find very poor matching; everything is assigned classe = B or E.</p>\r\n<pre class=\"r\"><code>eurico &lt;- training[training$user_name==&quot;eurico&quot;,c(8:11,37:49,60:68,84:86,113:124,151:160)]\r\nconfusionMatrix(eurico$classe,predict(modFit1,eurico))</code></pre>\r\n<pre><code>## Confusion Matrix and Statistics\r\n## \r\n##           Reference\r\n## Prediction   A   B   C   D   E\r\n##          A   0 218   0   0 647\r\n##          B   0  13   0   0 579\r\n##          C   0  29   0   0 460\r\n##          D   0   3   0   0 579\r\n##          E   0  25   0   0 517\r\n## \r\n## Overall Statistics\r\n##                                           \r\n##                Accuracy : 0.1726          \r\n##                  95% CI : (0.1594, 0.1865)\r\n##     No Information Rate : 0.9062          \r\n##     P-Value [Acc &gt; NIR] : 1               \r\n##                                           \r\n##                   Kappa : -0.0066         \r\n##  Mcnemar's Test P-Value : NA              \r\n## \r\n## Statistics by Class:\r\n## \r\n##                      Class: A Class: B Class: C Class: D Class: E\r\n## Sensitivity                NA 0.045139       NA       NA   0.1858\r\n## Specificity            0.7182 0.791876   0.8407   0.8104   0.9132\r\n## Pos Pred Value             NA 0.021959       NA       NA   0.9539\r\n## Neg Pred Value             NA 0.889023       NA       NA   0.1040\r\n## Prevalence             0.0000 0.093811   0.0000   0.0000   0.9062\r\n## Detection Rate         0.0000 0.004235   0.0000   0.0000   0.1684\r\n## Detection Prevalence   0.2818 0.192834   0.1593   0.1896   0.1765\r\n## Balanced Accuracy          NA 0.418508       NA       NA   0.5495</code></pre>\r\n</div>\r\n<div id=\"modeling-the-normalised-standardised-data-for-carlitos\" class=\"section level3\">\r\n<h3>Modeling the normalised, standardised data for Carlitos</h3>\r\n<p>Using the preProcess command, we can normalise and standardise the data for each subject, then training a model based on that .</p>\r\n<pre class=\"r\"><code>preProc &lt;- preProcess(carlitos[,-51],method=c(&quot;center&quot;,&quot;scale&quot;))\r\ncarlitos_norm &lt;- predict(preProc,carlitos[,-51])\r\nset.seed(5464)\r\nmodFit2 &lt;- train(factor(carlitos$classe)~.,method=&quot;rf&quot;,data=carlitos_norm)\r\n\r\n#  Save model here to avoid re running it in multiple compiles\r\nsaveRDS(modFit2,file=&quot;mod2&quot;)\r\nmodFit2&lt;- readRDS(&quot;mod2&quot;)\r\n\r\nconfusionMatrix(carlitos$classe,predict(modFit2,carlitos_norm))</code></pre>\r\n<pre><code>## Confusion Matrix and Statistics\r\n## \r\n##           Reference\r\n## Prediction   A   B   C   D   E\r\n##          A 834   0   0   0   0\r\n##          B   0 690   0   0   0\r\n##          C   0   0 493   0   0\r\n##          D   0   0   0 486   0\r\n##          E   0   0   0   0 609\r\n## \r\n## Overall Statistics\r\n##                                      \r\n##                Accuracy : 1          \r\n##                  95% CI : (0.9988, 1)\r\n##     No Information Rate : 0.268      \r\n##     P-Value [Acc &gt; NIR] : &lt; 2.2e-16  \r\n##                                      \r\n##                   Kappa : 1          \r\n##  Mcnemar's Test P-Value : NA         \r\n## \r\n## Statistics by Class:\r\n## \r\n##                      Class: A Class: B Class: C Class: D Class: E\r\n## Sensitivity             1.000   1.0000   1.0000   1.0000   1.0000\r\n## Specificity             1.000   1.0000   1.0000   1.0000   1.0000\r\n## Pos Pred Value          1.000   1.0000   1.0000   1.0000   1.0000\r\n## Neg Pred Value          1.000   1.0000   1.0000   1.0000   1.0000\r\n## Prevalence              0.268   0.2217   0.1584   0.1562   0.1957\r\n## Detection Rate          0.268   0.2217   0.1584   0.1562   0.1957\r\n## Detection Prevalence    0.268   0.2217   0.1584   0.1562   0.1957\r\n## Balanced Accuracy       1.000   1.0000   1.0000   1.0000   1.0000</code></pre>\r\n<pre class=\"r\"><code>#Compare with other, making sure to use SD and mean from sample\r\neurico_norm &lt;- predict(preProc,eurico[,-51])\r\nconfusionMatrix(eurico$classe,predict(modFit2,eurico_norm))</code></pre>\r\n<pre><code>## Confusion Matrix and Statistics\r\n## \r\n##           Reference\r\n## Prediction   A   B   C   D   E\r\n##          A   0 278   0   0 587\r\n##          B   0   9   0   0 583\r\n##          C   0   2   0   0 487\r\n##          D   0   2   0   0 580\r\n##          E   0  20   0   0 522\r\n## \r\n## Overall Statistics\r\n##                                           \r\n##                Accuracy : 0.173           \r\n##                  95% CI : (0.1597, 0.1868)\r\n##     No Information Rate : 0.8987          \r\n##     P-Value [Acc &gt; NIR] : 1               \r\n##                                           \r\n##                   Kappa : -0.0064         \r\n##  Mcnemar's Test P-Value : NA              \r\n## \r\n## Statistics by Class:\r\n## \r\n##                      Class: A Class: B Class: C Class: D Class: E\r\n## Sensitivity                NA 0.028939       NA       NA   0.1892\r\n## Specificity            0.7182 0.788692   0.8407   0.8104   0.9357\r\n## Pos Pred Value             NA 0.015203       NA       NA   0.9631\r\n## Neg Pred Value             NA 0.878128       NA       NA   0.1151\r\n## Prevalence             0.0000 0.101303   0.0000   0.0000   0.8987\r\n## Detection Rate         0.0000 0.002932   0.0000   0.0000   0.1700\r\n## Detection Prevalence   0.2818 0.192834   0.1593   0.1896   0.1765\r\n## Balanced Accuracy          NA 0.408815       NA       NA   0.5624</code></pre>\r\n<p>This is about the same.</p>\r\n</div>\r\n<div id=\"random-forest-of-whole-sample\" class=\"section level3\">\r\n<h3>Random forest of whole sample</h3>\r\n<p>The next step in model building was to make a random forest model for everybody involved. First, the entire sample was centered and scaled.</p>\r\n<pre class=\"r\"><code>trim &lt;- training[,c(8:11,37:49,60:68,84:86,113:124,151:160)]\r\npreProc &lt;- preProcess(trim[,-51],method=c(&quot;scale&quot;,&quot;center&quot;))\r\ntrim_prep &lt;- predict(preProc,trim[,-51])</code></pre>\r\n<p>Next, the training data were split into test and training subsets (3:1 split). We needed to make the same splits for the trimmed data and prepared data because the prepared data is missing the outcome column.</p>\r\n<pre class=\"r\"><code>inTrain &lt;-  createDataPartition(factor(trim$classe),p= 3/4)[[1]]\r\ntrim_train &lt;- training[inTrain,]\r\ntrim_test &lt;- training[-inTrain,]\r\n\r\nprep_train &lt;- trim_prep[inTrain,]\r\nprep_test &lt;- trim_prep[-inTrain,]</code></pre>\r\n<p>We created a new model, and tested it on the training set</p>\r\n<pre class=\"r\"><code>set.seed(5673)\r\nmodFit3 &lt;- train(factor(trim_train$classe)~.,method=&quot;rf&quot;,data=prep_train)\r\n\r\n#  Save model here to avoid re running it in multiple compiles\r\nsaveRDS(modFit3,file=&quot;mod3&quot;)\r\nmodFit3&lt;- readRDS(&quot;mod3&quot;)\r\nmodFit3$finalModel</code></pre>\r\n<pre><code>## \r\n## Call:\r\n##  randomForest(x = x, y = y, mtry = param$mtry) \r\n##                Type of random forest: classification\r\n##                      Number of trees: 500\r\n## No. of variables tried at each split: 2\r\n## \r\n##         OOB estimate of  error rate: 0.64%\r\n## Confusion matrix:\r\n##      A    B    C    D    E  class.error\r\n## A 4183    1    0    0    1 0.0004778973\r\n## B   11 2832    5    0    0 0.0056179775\r\n## C    0   14 2547    6    0 0.0077911959\r\n## D    0    0   42 2367    3 0.0186567164\r\n## E    0    0    2    9 2695 0.0040650407</code></pre>\r\n<pre class=\"r\"><code>confusionMatrix(trim_train$classe,predict(modFit3,prep_train))</code></pre>\r\n<pre><code>## Confusion Matrix and Statistics\r\n## \r\n##           Reference\r\n## Prediction    A    B    C    D    E\r\n##          A 4185    0    0    0    0\r\n##          B    0 2848    0    0    0\r\n##          C    0    0 2567    0    0\r\n##          D    0    0    0 2412    0\r\n##          E    0    0    0    0 2706\r\n## \r\n## Overall Statistics\r\n##                                      \r\n##                Accuracy : 1          \r\n##                  95% CI : (0.9997, 1)\r\n##     No Information Rate : 0.2843     \r\n##     P-Value [Acc &gt; NIR] : &lt; 2.2e-16  \r\n##                                      \r\n##                   Kappa : 1          \r\n##  Mcnemar's Test P-Value : NA         \r\n## \r\n## Statistics by Class:\r\n## \r\n##                      Class: A Class: B Class: C Class: D Class: E\r\n## Sensitivity            1.0000   1.0000   1.0000   1.0000   1.0000\r\n## Specificity            1.0000   1.0000   1.0000   1.0000   1.0000\r\n## Pos Pred Value         1.0000   1.0000   1.0000   1.0000   1.0000\r\n## Neg Pred Value         1.0000   1.0000   1.0000   1.0000   1.0000\r\n## Prevalence             0.2843   0.1935   0.1744   0.1639   0.1839\r\n## Detection Rate         0.2843   0.1935   0.1744   0.1639   0.1839\r\n## Detection Prevalence   0.2843   0.1935   0.1744   0.1639   0.1839\r\n## Balanced Accuracy      1.0000   1.0000   1.0000   1.0000   1.0000</code></pre>\r\n<p>Again we saw 100% success rate. We then tried it on the testing subset of the training data (which has the same standard deviation and mean for each column).</p>\r\n<pre class=\"r\"><code>confusionMatrix(trim_test$classe,predict(modFit3,prep_test))</code></pre>\r\n<pre><code>## Confusion Matrix and Statistics\r\n## \r\n##           Reference\r\n## Prediction    A    B    C    D    E\r\n##          A 1394    1    0    0    0\r\n##          B    2  944    3    0    0\r\n##          C    0    5  846    4    0\r\n##          D    0    0   15  789    0\r\n##          E    0    0    0    0  901\r\n## \r\n## Overall Statistics\r\n##                                           \r\n##                Accuracy : 0.9939          \r\n##                  95% CI : (0.9913, 0.9959)\r\n##     No Information Rate : 0.2847          \r\n##     P-Value [Acc &gt; NIR] : &lt; 2.2e-16       \r\n##                                           \r\n##                   Kappa : 0.9923          \r\n##  Mcnemar's Test P-Value : NA              \r\n## \r\n## Statistics by Class:\r\n## \r\n##                      Class: A Class: B Class: C Class: D Class: E\r\n## Sensitivity            0.9986   0.9937   0.9792   0.9950   1.0000\r\n## Specificity            0.9997   0.9987   0.9978   0.9964   1.0000\r\n## Pos Pred Value         0.9993   0.9947   0.9895   0.9813   1.0000\r\n## Neg Pred Value         0.9994   0.9985   0.9956   0.9990   1.0000\r\n## Prevalence             0.2847   0.1937   0.1762   0.1617   0.1837\r\n## Detection Rate         0.2843   0.1925   0.1725   0.1609   0.1837\r\n## Detection Prevalence   0.2845   0.1935   0.1743   0.1639   0.1837\r\n## Balanced Accuracy      0.9991   0.9962   0.9885   0.9957   1.0000</code></pre>\r\n<p>This is highly encouraging, with accuracy better than 99%, a huge improvement. However, this is likely to be too high, given that the training and test samples have the same means and standard deviations. In order to estimate out of sample error, we apply it to Carlitos' and Eurico's data, using the mean and standard deviations of the training sample. The means and standard deviations for individuals are expected to vary from those of the group.</p>\r\n<pre class=\"r\"><code># This pre processing uses whole training SDs and means\r\ncarlitos_prep &lt;- predict(preProc,carlitos[,-51])\r\noos_predict &lt;- predict(modFit3,carlitos_prep)\r\nconfusionMatrix(carlitos$classe,oos_predict)</code></pre>\r\n<pre><code>## Confusion Matrix and Statistics\r\n## \r\n##           Reference\r\n## Prediction   A   B   C   D   E\r\n##          A 834   0   0   0   0\r\n##          B   1 689   0   0   0\r\n##          C   0   1 492   0   0\r\n##          D   0   0   0 486   0\r\n##          E   0   0   0   0 609\r\n## \r\n## Overall Statistics\r\n##                                           \r\n##                Accuracy : 0.9994          \r\n##                  95% CI : (0.9977, 0.9999)\r\n##     No Information Rate : 0.2683          \r\n##     P-Value [Acc &gt; NIR] : &lt; 2.2e-16       \r\n##                                           \r\n##                   Kappa : 0.9992          \r\n##  Mcnemar's Test P-Value : NA              \r\n## \r\n## Statistics by Class:\r\n## \r\n##                      Class: A Class: B Class: C Class: D Class: E\r\n## Sensitivity            0.9988   0.9986   1.0000   1.0000   1.0000\r\n## Specificity            1.0000   0.9996   0.9996   1.0000   1.0000\r\n## Pos Pred Value         1.0000   0.9986   0.9980   1.0000   1.0000\r\n## Neg Pred Value         0.9996   0.9996   1.0000   1.0000   1.0000\r\n## Prevalence             0.2683   0.2217   0.1581   0.1562   0.1957\r\n## Detection Rate         0.2680   0.2214   0.1581   0.1562   0.1957\r\n## Detection Prevalence   0.2680   0.2217   0.1584   0.1562   0.1957\r\n## Balanced Accuracy      0.9994   0.9991   0.9998   1.0000   1.0000</code></pre>\r\n<p>Now try Eurico</p>\r\n<pre class=\"r\"><code>eurico_prep &lt;- predict(preProc,eurico[,-51])\r\noos_predict2 &lt;- predict(modFit3,eurico_prep)\r\nconfusionMatrix(eurico$classe,oos_predict2)</code></pre>\r\n<pre><code>## Confusion Matrix and Statistics\r\n## \r\n##           Reference\r\n## Prediction   A   B   C   D   E\r\n##          A 864   1   0   0   0\r\n##          B   0 592   0   0   0\r\n##          C   0   1 488   0   0\r\n##          D   0   0   0 582   0\r\n##          E   0   0   0   0 542\r\n## \r\n## Overall Statistics\r\n##                                           \r\n##                Accuracy : 0.9993          \r\n##                  95% CI : (0.9976, 0.9999)\r\n##     No Information Rate : 0.2814          \r\n##     P-Value [Acc &gt; NIR] : &lt; 2.2e-16       \r\n##                                           \r\n##                   Kappa : 0.9992          \r\n##  Mcnemar's Test P-Value : NA              \r\n## \r\n## Statistics by Class:\r\n## \r\n##                      Class: A Class: B Class: C Class: D Class: E\r\n## Sensitivity            1.0000   0.9966   1.0000   1.0000   1.0000\r\n## Specificity            0.9995   1.0000   0.9996   1.0000   1.0000\r\n## Pos Pred Value         0.9988   1.0000   0.9980   1.0000   1.0000\r\n## Neg Pred Value         1.0000   0.9992   1.0000   1.0000   1.0000\r\n## Prevalence             0.2814   0.1935   0.1590   0.1896   0.1765\r\n## Detection Rate         0.2814   0.1928   0.1590   0.1896   0.1765\r\n## Detection Prevalence   0.2818   0.1928   0.1593   0.1896   0.1765\r\n## Balanced Accuracy      0.9998   0.9983   0.9998   1.0000   1.0000</code></pre>\r\n</div>\r\n<div id=\"estimating-the-uncertainties\" class=\"section level3\">\r\n<h3>Estimating the uncertainties</h3>\r\n<p>The random forest method yields several estimates of the out of sample uncertainties. The first is the \"out of bag\" uncertaintity generated by the random forest model itself, 0.66%. We can also look at the accuracies of the model fits for individual data (1 - uncertainty), since the means and standard deviations will differ from those used in generating the model. The tests with Carlitos' and Eurico's data, yield 95% confdidence lower limits for accuracy of 99.9% and 99.62% respectively, for 0.1% and 0.38% uncertaincy respectively. We assume the largest uncertainty of these, 0.66%.</p>\r\n</div>\r\n</div>\r\n<div id=\"running-the-test.\" class=\"section level2\">\r\n<h2>Running the test.</h2>\r\n<p>We now filter the test set, then scale and centre the test data.</p>\r\n<pre class=\"r\"><code>test &lt;- testing[,c(8:11,37:49,60:68,84:86,113:124,151:160)]\r\ntest_prep &lt;- predict(preProc,test[,-51])\r\npreds &lt;- predict(modFit3,test_prep)</code></pre>\r\n<p>Cue fanfare here are the predictions:</p>\r\n<pre class=\"r\"><code>preds</code></pre>\r\n<pre><code>##  [1] B A B A A E D B A A B C B A E E A B B B\r\n## Levels: A B C D E</code></pre>\r\n<p>All that remains is to write the funtion to get them ready for submission.</p>\r\n<pre class=\"r\"><code>pml_write_files = function(x){\r\n  n = length(x)\r\n  for(i in 1:n){\r\n    filename = paste0(&quot;problem_id_&quot;,i,&quot;.txt&quot;)\r\n    write.table(x[i],file=filename,quote=FALSE,row.names=FALSE,col.names=FALSE)\r\n  }\r\n}\r\npml_write_files(preds)</code></pre>\r\n<p>We scored 100%!</p>\r\n</div>\r\n</div>\r\n\r\n\r\n</div>\r\n\r\n<script>\r\n\r\n// add bootstrap table styles to pandoc tables\r\n$(document).ready(function () {\r\n  $('tr.header').parent('thead').parent('table').addClass('table table-condensed');\r\n});\r\n\r\n</script>\r\n\r\n<!-- dynamically load mathjax for compatibility with self-contained -->\r\n<script>\r\n  (function () {\r\n    var script = document.createElement(\"script\");\r\n    script.type = \"text/javascript\";\r\n    script.src  = \"https://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML\";\r\n    document.getElementsByTagName(\"head\")[0].appendChild(script);\r\n  })();\r\n</script>\r\n\r\n</body>\r\n</html>\r\n","google":"","note":"Don't delete this file! It's used internally to help with page regeneration."}